{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "python nlp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY7l5lU_QnSr"
      },
      "source": [
        "**NLP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmXi1xO0QvaO"
      },
      "source": [
        "NLP – это раздел Data Science , касающийся широкого спектра задач по обработке естественного языка."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtmrsO9vQym4"
      },
      "source": [
        "Рассмотрим некоторые задачи , которые решает NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4z10jttQ7eJ"
      },
      "source": [
        "1.\tКлассификация текстов. Примером такой задачи может являться разделение новостей в ленте социальных сетей по рубрикам – IT , спорт, мода и т.д. \n",
        "2.\tЧат-бот и другие диалоговые системы , основанные на искусственном интеллекте. Данная задача является одной из самых «модных» в NLP. Примером является голосовой помощник Олег от Тинькофф банка, а также Siri от Apple. Текстовая классификация позволяет голосовым помощникам определить дальнейший сценарий взаимодействия с человеком. Например , пользователь хочет узнать о погоде и чат-бот после обработки текста выдает ответ пользователю.\n",
        "3.\tСаммаризация. Данная задача представляет собой «пересказ» поступившего на вход программы текста. Данный процесс необходим , т.к в  ходе обучения нейронной сети могут  появляться слова , имеющие одно значение , но , например, разные падежи. Саммаризация убирает одинаковые слова, не несущие смыслого значения.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRDGGJtMRAg3"
      },
      "source": [
        "**Методы решения** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n6imENvRHc6"
      },
      "source": [
        "Важнейшую роль в NLP играет предобработка данных. Данный метод подразделяется на несколько шагов: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-2gEUAeRP_q"
      },
      "source": [
        "1.\tУдаление специальных символов\n",
        "2.\tПриведение текста к нижнему регистру\n",
        "3.\tПреобразование чисел\n",
        "4.\tУдаление стоп-слов\n",
        "5.\tЛематизация\n",
        "6.\tТокенизация\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsNzXnkbRT91"
      },
      "source": [
        "Рассмотрим каждый этап более детально."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwHgqDQBOs19"
      },
      "source": [
        "1.Для удаления специальных символов используется модуль для работы с регулярными выражениями ‘re’. Основной функцией из этого модуля является функция `re.sub`, которая ищет фрагменты строки по заданному шаблону и заменяет на указанную подстроку. Давайте посмотрим работу этой функции на примере."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n6IyavlQi40"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpl1xX0KOxWN"
      },
      "source": [
        "import re # Импортируем модуль для работы с регулярными выражениями\n",
        "\n",
        "test_str = '<p>Hello <b>World!!!</b><p>'\n",
        "print(test_str)\n",
        "\n",
        "# Используем функцию re.sub. Первый ее аргумент - шаблон строки, по которому нужно искать подстроки.\n",
        "# Второй аргумент - строка, на которую нужно заменить найденную подстроку. Третий аргумент - строка, \n",
        "# в которой нужно искать и заменять подстроки.\n",
        "clear_str = re.sub(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', test_str)\n",
        "print(clear_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ4DmQTMO7p8"
      },
      "source": [
        "Часто входными данными служат документы HTML, получающиеся в ходе парсинга сайта. Дана строка, содержащая HTML-тэги, и нам необходимо удалить эти тэги из строки. Понятно, что подобным же образом можно искать и заменять любые подстроки в строке, например удалять гиперссылки, заменять числа на специальный токен и т.д."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eL6KGLOO_jk"
      },
      "source": [
        "2.Токенизация строк. Токенизация - это разбиение текста на отдельные составляющие (слова, символы, знаки препинания). Для этого применим еще одну внешнюю библиотеку stanza. Чтобы ее установить, нужно набрать в терминале `pip install stanza`.Еще одной библиотекой для токенизации текста является nltk. Для разделения текста на отдельные токены нам понадобится функция `nltk.tokenize.word_tokenize`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n53N7FlbPJV8"
      },
      "source": [
        "import stanza # Импортируем библиотеку stanza\n",
        "# Чтобы использовать библиотеку для английского языка, нужно скачать набор моделей.\n",
        "stanza.download('en')\n",
        "\n",
        "test_str = '''Stanza is a Python natural language analysis package. \n",
        "It contains tools, which can be used in a pipeline, to convert a string containing human language \n",
        "text into lists of sentences and words, to generate base forms of those words, \n",
        "their parts of speech and morphological features, to give a syntactic structure dependency parse, \n",
        "and to recognize named entities. The toolkit is designed to be parallel among more than 60 languages, \n",
        "using the Universal Dependencies formalism.'''\n",
        "print('Исходный текст: {}'.format(test_str))\n",
        "\n",
        "\n",
        "# Создадим обработчик для английского языка, который будет включать в себя токенизатор\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
        "\n",
        "# Передаем в созданный обработчик заданный текст и получаем на выходе обработанный текст, \n",
        "# записанный в определенной структуре.\n",
        "doc = nlp(test_str)\n",
        "\n",
        "tokens = [] # Создадим список, в котором будем хранить отдельные слова\n",
        "for sent in doc.sentences: # Перебираем предложения\n",
        "    for word in sent.words: # Перебираем все слова в каждом из предложений\n",
        "        tokens.append(word.text) # Чтобы получить доступ к слову, необходимо обратиться к нему word.text\n",
        "\n",
        "tokenized_str = ' '.join(tokens) # Объединяем отдельные слова в один текст, разделяя слова пробелами\n",
        "print('Токенизированный текст: {}'.format(tokenized_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rc4_uKkPMUH"
      },
      "source": [
        "3. Приведение текста к нижнему регистру. Часто одни и те же слова могут начинаться с маленькой и большой букв(например, в начале предложений). Чтобы модель не делала различий между этими двумя вариантами, нужно приводить текст к нижнему регистру. Для этого используется встроенный в Python метод - `lower`. Рассмотрим как приводить слова к нижнему регистру, когда вы используете библиотеку stanza и когда разделяете предложение с помощью `split`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBTbgyHkPhGk"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(test_str)\n",
        "\n",
        "tokenized_str = ' '.join(tokens)\n",
        "print('Токенизированный текст: {}'.format(tokenized_str))\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "test_str = '''Stanza is a Python natural language analysis package. \n",
        "It contains tools, which can be used in a pipeline, to convert a string containing human language \n",
        "text into lists of sentences and words, to generate base forms of those words, \n",
        "their parts of speech and morphological features, to give a syntactic structure dependency parse, \n",
        "and to recognize named entities. The toolkit is designed to be parallel among more than 60 languages, \n",
        "using the Universal Dependencies formalism.'''\n",
        "print('Исходный текст: {}'.format(test_str))\n",
        "\n",
        "doc = nlp(test_str)\n",
        "\n",
        "tokens = []\n",
        "for sent in doc.sentences:\n",
        "    for word in sent.words:\n",
        "        tokens.append(word.text.lower()) # Приведение к нижнему регистру\n",
        "\n",
        "tokenized_str = ' '.join(tokens)\n",
        "print('Токенизированный текст: {}'.format(tokenized_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1C_Mg4lPoqm"
      },
      "source": [
        "4. Преобразование чисел. Часто в тексте встречаются числа. Моделям не обязательно знать, что это за числа. Поэтому, чтобы система не делала различий между числами, принято заменять их специальным токеном, например `__NUM__`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-Z0S557PwDE"
      },
      "source": [
        "test_str = '''Stanza is a Python natural language analysis package. \n",
        "It contains tools, which can be used in a pipeline, to convert a string containing human language \n",
        "text into lists of sentences and words, to generate base forms of those words, \n",
        "their parts of speech and morphological features, to give a syntactic structure dependency parse, \n",
        "and to recognize named entities. The toolkit is designed to be parallel among more than 60 languages, \n",
        "using the Universal Dependencies formalism.'''\n",
        "print('Исходный текст: {}'.format(test_str))\n",
        "\n",
        "# Помимо токенизатора используем модули, отвечающие за определение части речи. \n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos')\n",
        "\n",
        "doc = nlp(test_str)\n",
        "\n",
        "tokens = []\n",
        "for sent in doc.sentences:\n",
        "    for word in sent.words:\n",
        "        # Все числа имеюбт часть речи NUM\n",
        "        # А если вы захотите удалить все знаки пунктуации, то вам поможет часть речи PUNCT\n",
        "        if word.upos == 'NUM':\n",
        "            tokens.append('__NUM__') # Заменяем все числа на специальный токен\n",
        "        else:\n",
        "            tokens.append(word.text.lower()) # Если нам встретилось не число, то помещаем его в список\n",
        "\n",
        "tokenized_str = ' '.join(tokens)\n",
        "print('Токенизированный текст без чисел: {}'.format(tokenized_str))\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "swords = stopwords.words('english')\n",
        "print(swords)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hVufmgFP09n"
      },
      "source": [
        "5. Удаление стоп-слов. Для некоторых задач обработки естественного языка часть слов не только не несет какой-то значимой информации, но и мешают работе моделей. Примеры таких слов для английского языка: a, an, the, at, in и т.д. Весь список стоп-слов для конкретного языка можно увидеть, вызвав `nltk.corpus.stopwords.words('english')`. Рассмотрим, как удалить стоп-слова при обработке текста библиотекой stanza и при разделении текста с помощью функции split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDOKdVOvQDLj"
      },
      "source": [
        "test_str = '''Stanza is a Python natural language analysis package. \n",
        "It contains tools, which can be used in a pipeline, to convert a string containing human language \n",
        "text into lists of sentences and words, to generate base forms of those words, \n",
        "their parts of speech and morphological features, to give a syntactic structure dependency parse, \n",
        "and to recognize named entities. The toolkit is designed to be parallel among more than 60 languages, \n",
        "using the Universal Dependencies formalism.'''\n",
        "print('Исходный текст: {}'.format(test_str))\n",
        "\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos')\n",
        "\n",
        "doc = nlp(test_str)\n",
        "\n",
        "tokens = []\n",
        "for sent in doc.sentences:\n",
        "    for word in sent.words:\n",
        "        if word.upos == 'NUM':\n",
        "            tokens.append('__NUM__')\n",
        "        else:\n",
        "            token = word.text.lower()\n",
        "            if token not in swords:\n",
        "                tokens.append(token)\n",
        "\n",
        "tokenized_str = ' '.join(tokens)\n",
        "print('Токенизированный текст без чисел: {}'.format(tokenized_str))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoIjzTrxQSHe"
      },
      "source": [
        "6. Лематизация. Часто в языках одни и те же слова имеют различные словоформы. Например, для русского языка - дом, дома, дому и т.д. Модели машинного обучения нужно понимать, что все эти формы слова означают одно и тоже. Для этого используют лематизацию - приведение слов к изначальной форме."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZniPaFxBQYJP"
      },
      "source": [
        "test_str = '''Stanza is a Python natural language analysis package. \n",
        "It contains tools, which can be used in a pipeline, to convert a string containing human language \n",
        "text into lists of sentences and words, to generate base forms of those words, \n",
        "their parts of speech and morphological features, to give a syntactic structure dependency parse, \n",
        "and to recognize named entities. The toolkit is designed to be parallel among more than 60 languages, \n",
        "using the Universal Dependencies formalism.'''\n",
        "print('Исходный текст: {}'.format(test_str))\n",
        "\n",
        "# Добавляем обработчик, ответственный за нахождение изначальных форм слов\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
        "\n",
        "doc = nlp(test_str)\n",
        "\n",
        "tokens = []\n",
        "for sent in doc.sentences:\n",
        "    for word in sent.words:\n",
        "        if word.upos == 'NUM':\n",
        "            tokens.append('__NUM__')\n",
        "        else:\n",
        "            token = word.lemma.lower() # Находим изначальную форму слова\n",
        "            if token not in swords:\n",
        "                tokens.append(token)\n",
        "\n",
        "tokenized_str = ' '.join(tokens)\n",
        "print('Токенизированный текст без чисел: {}'.format(tokenized_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDmK4UuARYdc"
      },
      "source": [
        "**Инструменты**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUoFVZSKRcKW"
      },
      "source": [
        "Инструментами разработчика искусственного интеллекта в сфере NLP являются библиотеки и фрейворки. PyTorch , Tensorflow, Scikit-learn являются основными инструментами. В качестве IDE используется Jypiter Notebook или Google Colab."
      ]
    }
  ]
}